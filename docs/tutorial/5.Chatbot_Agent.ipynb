{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index==0.11.4\n",
    "# !pip install PyYAML\n",
    "# !pip install plotly==5.24.0\n",
    "# !pip install docx2txt==0.8\n",
    "# !pip install chromadb==0.5.5\n",
    "# !pip install llama-index-vector-stores-chroma==0.2.0\n",
    "# !pip install llama-index-extractors-entity==0.2.0\n",
    "# !pip install llama-index-readers-web==0.2.2\n",
    "# !pip install transformers==4.40.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-your-api-key\"\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiemdinh/miniconda3/envs/test-mh-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:phoenix.config:üìã Ensuring phoenix working directory: /home/tiemdinh/.phoenix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import set_global_handler\n",
    "import phoenix as px\n",
    "\n",
    "px.launch_app()\n",
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√®o √ö c·ªßa b·∫°n ƒëang ng·ªß b√™n c·ª≠a s·ªï.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "text = \"T√¥i c√≥ m·ªôt b√© m√®o, c·∫≠u ·∫•y t√™n l√† M√®o √ö. C·∫≠u ·∫•y ƒëang ng·ªß b√™n c·ª≠a s·ªï. T√¥i r·∫•t qu√Ω c·∫≠u ·∫•y.\"\n",
    "doc = Document(text=text)\n",
    "index = VectorStoreIndex.from_documents([doc])\n",
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\"T√¥i l√† Ti·ªÅm, m√®o √ö c·ªßa t√¥i ƒëang l√†m g√¨ nh·ªâ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i l√† Ti·ªÅm, m√®o √ö c·ªßa t√¥i ƒëang l√†m g√¨ nh·ªâ?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_ELGpkWmYBxR8LQyqyajSJK9B', function=Function(arguments='{\"input\":\"M√®o √ö c·ªßa Ti·ªÅm ƒëang l√†m g√¨?\"}', name='query_engine_tool'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='M√®o √ö ƒëang ng·ªß b√™n c·ª≠a s·ªï.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_ELGpkWmYBxR8LQyqyajSJK9B'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='M√®o √ö c·ªßa b·∫°n ƒëang ng·ªß b√™n c·ª≠a s·ªï.', additional_kwargs={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫°n t√™n l√† Ti·ªÅm.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"T√¥i t√™n l√† g√¨?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i l√† Ti·ªÅm, m√®o √ö c·ªßa t√¥i ƒëang l√†m g√¨ nh·ªâ?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_yQOxBuLn9WWLzsuad3Vtz4rM', function=Function(arguments='{\"input\":\"M√®o √ö ƒëang l√†m g√¨?\"}', name='query_engine_tool'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='M√®o √ö ƒëang ng·ªß b√™n c·ª≠a s·ªï.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_yQOxBuLn9WWLzsuad3Vtz4rM'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='M√®o √ö c·ªßa b·∫°n ƒëang ng·ªß b√™n c·ª≠a s·ªï.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i t√™n l√† g√¨?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='B·∫°n t√™n l√† Ti·ªÅm.', additional_kwargs={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√≥ v·∫ª nh∆∞ b·∫°n ƒëang mu·ªën bi·∫øt v·ªÅ t√¨nh c·∫£m c·ªßa m√¨nh ƒë·ªëi v·ªõi M√®o √ö. D∆∞·ªõi ƒë√¢y l√† m·ªôt b√†i th∆° th·ªÉ hi·ªán t√¨nh y√™u d√†nh cho M√®o √ö:\n",
      "\n",
      "M√®o √ö nh·ªè b√©, ƒë√°ng y√™u v√¥ c√πng,  \n",
      "Trong l√≤ng t√¥i, c·∫≠u l√† √°nh trƒÉng r·∫±m.  \n",
      "M·ªói bu·ªïi s√°ng, c·∫≠u nh·∫£y nh√≥t vui,  \n",
      "Mang ƒë·∫øn n·ª• c∆∞·ªùi, xua tan u √°m.\n",
      "\n",
      "L√¥ng m·ªÅm m·∫°i, nh∆∞ m√¢y tr·ªùi bay,  \n",
      "√Ånh m·∫Øt trong veo, nh∆∞ n∆∞·ªõc h·ªì ƒë·∫ßy.  \n",
      "C·∫≠u l√† b·∫°n th√¢n, l√† ni·ªÅm an ·ªßi,  \n",
      "M·ªói ph√∫t gi√¢y b√™n nhau, th·∫≠t tuy·ªát v·ªùi.\n",
      "\n",
      "Khi c·∫≠u ng·ªß say, t√¥i ng·∫Øm nh√¨n,  \n",
      "T√¨nh y√™u d√†nh cho c·∫≠u, m√£i kh√¥ng phai.  \n",
      "M√®o √ö ∆°i, c·∫≠u l√† m√≥n qu√†,  \n",
      "Trong tr√°i tim t√¥i, c·∫≠u lu√¥n l√† nh·∫•t."
     ]
    }
   ],
   "source": [
    "response = chat_engine.stream_chat(\"T√¥i c√≥ y√™u c·∫≠u ·∫•y kh√¥ng? Vi·∫øt m·ªôt b√†i th∆° v·ªÅ M√®o √ö ƒëi.\")\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i l√† Ti·ªÅm, m√®o √ö c·ªßa t√¥i ƒëang l√†m g√¨ nh·ªâ?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_SztjRMkbEtABahzyhVb5CSGU', function=Function(arguments='{\"input\":\"M√®o √ö c·ªßa Ti·ªÅm ƒëang l√†m g√¨?\"}', name='query_engine_tool'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='M√®o √ö ƒëang ng·ªß b√™n c·ª≠a s·ªï.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_SztjRMkbEtABahzyhVb5CSGU'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='M√®o √ö c·ªßa b·∫°n ƒëang ng·ªß b√™n c·ª≠a s·ªï.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i t√™n l√† g√¨?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='B·∫°n t√™n l√† Ti·ªÅm.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i c√≥ y√™u c·∫≠u ·∫•y kh√¥ng? Vi·∫øt m·ªôt b√†i th∆° v·ªÅ M√®o √ö ƒëi.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='', additional_kwargs={'tool_calls': [ChoiceDeltaToolCall(index=0, id='call_DLFF5GI6IKdnXFn8bPejDkmT', function=ChoiceDeltaToolCallFunction(arguments='{\"input\":\"Vi·∫øt m·ªôt b√†i th∆° v·ªÅ M√®o √ö\"}', name='query_engine_tool'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='M√®o √ö n·∫±m b√™n c·ª≠a s·ªï,  \\nGi·∫•c m∆° bay b·ªïng, nh·∫π nh√†ng tr√¥i.  \\nL√¥ng m·ªÅm m·∫°i, √°nh n·∫Øng r·ª±c r·ª°,  \\nC·∫≠u ·∫•y l√† ni·ªÅm vui trong ƒë·ªùi.  \\n\\nM·∫Øt nh·∫Øm nghi·ªÅn, m∆° v·ªÅ c√°nh ƒë·ªìng,  \\nCh·∫°y nh·∫£y vui ƒë√πa, kh√¥ng lo √¢u s·∫ßu.  \\nT√¥i y√™u c·∫≠u, t√¨nh th∆∞∆°ng v√¥ b·ªù,  \\nM√®o √ö ∆°i, m√£i m√£i b√™n nhau.  \\n\\nKhi th·ª©c d·∫≠y, c·∫≠u s·∫Ω ngh·ªãch ng·ª£m,  \\nCh·∫°y quanh nh√†, ƒëu·ªïi theo b√≥ng m·ªù.  \\nM·ªói ng√†y tr√¥i, th√™m nhi·ªÅu k·ª∑ ni·ªám,  \\nM√®o √ö ∆°i, c·∫≠u l√† m√≥n qu√†.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_DLFF5GI6IKdnXFn8bPejDkmT'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='D∆∞·ªõi ƒë√¢y l√† b√†i th∆° v·ªÅ M√®o √ö:\\n\\nM√®o √ö n·∫±m b√™n c·ª≠a s·ªï,  \\nGi·∫•c m∆° bay b·ªïng, nh·∫π nh√†ng tr√¥i.  \\nL√¥ng m·ªÅm m·∫°i, √°nh n·∫Øng r·ª±c r·ª°,  \\nC·∫≠u ·∫•y l√† ni·ªÅm vui trong ƒë·ªùi.  \\n\\nM·∫Øt nh·∫Øm nghi·ªÅn, m∆° v·ªÅ c√°nh ƒë·ªìng,  \\nCh·∫°y nh·∫£y vui ƒë√πa, kh√¥ng lo √¢u s·∫ßu.  \\nT√¥i y√™u c·∫≠u, t√¨nh th∆∞∆°ng v√¥ b·ªù,  \\nM√®o √ö ∆°i, m√£i m√£i b√™n nhau.  \\n\\nKhi th·ª©c d·∫≠y, c·∫≠u s·∫Ω ngh·ªãch ng·ª£m,  \\nCh·∫°y quanh nh√†, ƒëu·ªïi theo b√≥ng m·ªù.  \\nM·ªói ng√†y tr√¥i, th√™m nhi·ªÅu k·ª∑ ni·ªám,  \\nM√®o √ö ∆°i, c·∫≠u l√† m√≥n qu√†.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i c√≥ y√™u c·∫≠u ·∫•y kh√¥ng? Vi·∫øt m·ªôt b√†i th∆° v·ªÅ M√®o √ö ƒëi.', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='C√≥ v·∫ª nh∆∞ b·∫°n ƒëang mu·ªën bi·∫øt v·ªÅ t√¨nh c·∫£m c·ªßa m√¨nh ƒë·ªëi v·ªõi M√®o √ö. D∆∞·ªõi ƒë√¢y l√† m·ªôt b√†i th∆° th·ªÉ hi·ªán t√¨nh y√™u d√†nh cho M√®o √ö:\\n\\nM√®o √ö nh·ªè b√©, ƒë√°ng y√™u v√¥ c√πng,  \\nTrong l√≤ng t√¥i, c·∫≠u l√† √°nh trƒÉng r·∫±m.  \\nM·ªói bu·ªïi s√°ng, c·∫≠u nh·∫£y nh√≥t vui,  \\nMang ƒë·∫øn n·ª• c∆∞·ªùi, xua tan u √°m.\\n\\nL√¥ng m·ªÅm m·∫°i, nh∆∞ m√¢y tr·ªùi bay,  \\n√Ånh m·∫Øt trong veo, nh∆∞ n∆∞·ªõc h·ªì ƒë·∫ßy.  \\nC·∫≠u l√† b·∫°n th√¢n, l√† ni·ªÅm an ·ªßi,  \\nM·ªói ph√∫t gi√¢y b√™n nhau, th·∫≠t tuy·ªát v·ªùi.\\n\\nKhi c·∫≠u ng·ªß say, t√¥i ng·∫Øm nh√¨n,  \\nT√¨nh y√™u d√†nh cho c·∫≠u, m√£i kh√¥ng phai.  \\nM√®o √ö ∆°i, c·∫≠u l√† m√≥n qu√†,  \\nTrong tr√°i tim t√¥i, c·∫≠u lu√¥n l√† nh·∫•t.', additional_kwargs={})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat_repl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: Ch√†o b·∫°n! B·∫°n c·∫ßn gi√∫p g√¨ h√¥m nay?\n",
      "\n",
      "Assistant: M√®o √∫ th∆∞·ªùng ƒë∆∞·ª£c bi·∫øt ƒë·∫øn v·ªõi v·∫ª ngo√†i d·ªÖ th∆∞∆°ng v√† t√≠nh c√°ch th√¢n thi·ªán. B·∫°n c√≥ k·ª∑ ni·ªám n√†o ƒë·∫∑c bi·ªát v·ªõi m√®o √∫ kh√¥ng? Ho·∫∑c b·∫°n mu·ªën bi·∫øt th√™m th√¥ng tin g√¨ v·ªÅ ch√∫ng?\n",
      "\n",
      "Assistant: M√®o √∫ ƒëang ng·ªß b√™n c·ª≠a s·ªï. C√≥ v·∫ª nh∆∞ c·∫≠u ·∫•y ƒëang t·∫≠n h∆∞·ªüng m·ªôt gi·∫•c ng·ªß ngon! B·∫°n c√≥ mu·ªën bi·∫øt th√™m ƒëi·ªÅu g√¨ v·ªÅ m√®o √∫ kh√¥ng?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='Ch√†o b·∫°n', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Ch√†o b·∫°n! B·∫°n c·∫ßn gi√∫p g√¨ h√¥m nay?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i nh·ªõ m√®o √∫', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='M√®o √∫ th∆∞·ªùng ƒë∆∞·ª£c bi·∫øt ƒë·∫øn v·ªõi v·∫ª ngo√†i d·ªÖ th∆∞∆°ng v√† t√≠nh c√°ch th√¢n thi·ªán. B·∫°n c√≥ k·ª∑ ni·ªám n√†o ƒë·∫∑c bi·ªát v·ªõi m√®o √∫ kh√¥ng? Ho·∫∑c b·∫°n mu·ªën bi·∫øt th√™m th√¥ng tin g√¨ v·ªÅ ch√∫ng?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='C·∫≠u ·∫•y ƒëang l√†m g√¨?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_tOzuSka3vdbSgGdB9VSuQ6sz', function=Function(arguments='{\"input\":\"M√®o √∫ ƒëang l√†m g√¨?\"}', name='query_engine_tool'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='M√®o √ö ƒëang ng·ªß b√™n c·ª≠a s·ªï.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_tOzuSka3vdbSgGdB9VSuQ6sz'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='M√®o √∫ ƒëang ng·ªß b√™n c·ª≠a s·ªï. C√≥ v·∫ª nh∆∞ c·∫≠u ·∫•y ƒëang t·∫≠n h∆∞·ªüng m·ªôt gi·∫•c ng·ªß ngon! B·∫°n c√≥ mu·ªën bi·∫øt th√™m ƒëi·ªÅu g√¨ v·ªÅ m√®o √∫ kh√¥ng?', additional_kwargs={})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: C√≥, b·∫°n r·∫•t qu√Ω m√®o.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='T√¥i c√≥ y√™u m√®o kh√¥ng?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_6sIbQjbsc9POkGhR6hdniFXU', function=Function(arguments='{\"input\":\"T√¥i c√≥ y√™u m√®o kh√¥ng?\"}', name='query_engine_tool'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='C√≥, b·∫°n r·∫•t qu√Ω m√®o.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_6sIbQjbsc9POkGhR6hdniFXU'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='C√≥, b·∫°n r·∫•t qu√Ω m√®o.', additional_kwargs={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X√≥a l·ªãch s·ª≠ chat\n",
    "chat_engine.reset()\n",
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C∆° ch·∫ø qu·∫£n l√≠ l·ªãch s·ª≠ chat\n",
    "\n",
    "## ChatMemoryBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.chat_store import SimpleChatStore\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "text = \"M√®o √ö ƒëi l·∫°c, m·∫•y ng√†y r·ªìi c·∫≠u ·∫•y kh√¥ng v·ªÅ nh√†, t√¥i lo qu√°!\"\n",
    "doc = Document(text=text)\n",
    "index = VectorStoreIndex.from_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chat_store = SimpleChatStore.from_persist_path(\n",
    "        persist_path = \"chat_memory.json\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    chat_store = SimpleChatStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(\n",
    "    token_limit= 20,\n",
    "    chat_store=chat_store,\n",
    "    chat_store_key=\"User 1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Xin ch√†o! B·∫°n c·∫ßn gi√∫p g√¨ h√¥m nay?\n",
      "Bot: M√®o √ö ƒëang ƒëi l·∫°c v√† ƒë√£ kh√¥ng v·ªÅ nh√† m·∫•y ng√†y r·ªìi.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    response = chat_engine.chat(user_input)\n",
    "    print(\"Bot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='Xin ch√†o', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Xin ch√†o! B·∫°n c·∫ßn gi√∫p g√¨ h√¥m nay?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='M√®o √ö ƒëang ·ªü ƒë√¢u?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_w1KN9B8ZsN8ZItPpWk4uBj9v', function=Function(arguments='{\"input\":\"M√®o √ö ƒëang ·ªü ƒë√¢u?\"}', name='query_engine_tool'), type='function')]}),\n",
       " ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='M√®o √ö ƒëi l·∫°c v√† ƒë√£ kh√¥ng v·ªÅ nh√† m·∫•y ng√†y r·ªìi.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_w1KN9B8ZsN8ZItPpWk4uBj9v'}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='M√®o √ö ƒëang ƒëi l·∫°c v√† ƒë√£ kh√¥ng v·ªÅ nh√† m·∫•y ng√†y r·ªìi.', additional_kwargs={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleChatStore(store={'User 1': [ChatMessage(role=<MessageRole.USER: 'user'>, content='Xin ch√†o', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Xin ch√†o! B·∫°n c·∫ßn gi√∫p g√¨ h√¥m nay?', additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='M√®o √ö ƒëang ·ªü ƒë√¢u?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=None, additional_kwargs={'tool_calls': [ChatCompletionMessageToolCall(id='call_w1KN9B8ZsN8ZItPpWk4uBj9v', function=Function(arguments='{\"input\":\"M√®o √ö ƒëang ·ªü ƒë√¢u?\"}', name='query_engine_tool'), type='function')]}), ChatMessage(role=<MessageRole.TOOL: 'tool'>, content='M√®o √ö ƒëi l·∫°c v√† ƒë√£ kh√¥ng v·ªÅ nh√† m·∫•y ng√†y r·ªìi.', additional_kwargs={'name': 'query_engine_tool', 'tool_call_id': 'call_w1KN9B8ZsN8ZItPpWk4uBj9v'}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='M√®o √ö ƒëang ƒëi l·∫°c v√† ƒë√£ kh√¥ng v·ªÅ nh√† m·∫•y ng√†y r·ªìi.', additional_kwargs={})]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store.persist(persist_path=\"chat_memory.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C√°c ch·∫ø ƒë·ªô tr√≤ chuy·ªán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch·∫ø ƒë·ªô ƒë∆°n gi·∫£n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: Xin ch√†o, Ti·ªÅm! R·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n. B·∫°n c√≥ ƒëi·ªÅu g√¨ mu·ªën chia s·∫ª ho·∫∑c h·ªèi kh√¥ng?\n",
      "\n",
      "Assistant: Nghe th·∫≠t tuy·ªát! Kem m√°t l·∫°nh l√† m√≥n ƒÉn gi·∫£i kh√°t tuy·ªát v·ªùi, ƒë·∫∑c bi·ªát trong nh·ªØng ng√†y n√≥ng. B·∫°n th√≠ch lo·∫°i kem n√†o nh·∫•t? Kem tr√°i c√¢y, kem s√¥ c√¥ la hay kem vani?\n",
      "\n",
      "Assistant: C√≥, b·∫°n ƒë√£ gi·ªõi thi·ªáu t√™n l√† Ti·ªÅm. N·∫øu b·∫°n mu·ªën chia s·∫ª th√™m v·ªÅ b·∫£n th√¢n ho·∫∑c c√≥ c√¢u h·ªèi n√†o kh√°c, h√£y cho t√¥i bi·∫øt nh√©!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults()\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch·∫ø ƒë·ªô ng·ªØ c·∫£nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "text = \"ƒê√™m qua ƒëi ch∆°i v·ªÅ mu·ªôn, m√®o √ö b·ªã s·∫≠p b·∫´y!\"\n",
    "doc = Document(text=text)\n",
    "index = VectorStoreIndex.from_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: Xin ch√†o! B·∫°n c·∫ßn gi√∫p g√¨ h√¥m nay? C√≥ ph·∫£i b·∫°n mu·ªën t√¨m hi·ªÉu th√™m v·ªÅ v·ª• vi·ªác m√®o √ö b·ªã b·∫Øt kh√¥ng?\n",
      "\n",
      "Assistant: T·ªët! ƒê·ªÉ t√¨m ra ai ƒë√£ b·∫Øt m√®o √ö, ch√∫ng ta c·∫ßn thu th·∫≠p m·ªôt s·ªë th√¥ng tin v√† ƒë·∫∑t ra m·ªôt s·ªë c√¢u h·ªèi. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë b∆∞·ªõc m√† ch√∫ng ta c√≥ th·ªÉ th·ª±c hi·ªán:\n",
      "\n",
      "1. **Th·ªùi gian v√† ƒë·ªãa ƒëi·ªÉm**: B·∫°n c√≥ bi·∫øt ch√≠nh x√°c th·ªùi gian v√† ƒë·ªãa ƒëi·ªÉm m√† m√®o √ö b·ªã b·∫Øt kh√¥ng? ƒêi·ªÅu n√†y c√≥ th·ªÉ gi√∫p x√°c ƒë·ªãnh ai c√≥ m·∫∑t ·ªü ƒë√≥ v√†o th·ªùi ƒëi·ªÉm ƒë√≥.\n",
      "\n",
      "2. **Nh·ªØng ng∆∞·ªùi xung quanh**: C√≥ ai ƒë√£ th·∫•y m√®o √ö ho·∫∑c c√≥ ai ƒë√£ ·ªü g·∫ßn khu v·ª±c ƒë√≥ v√†o th·ªùi ƒëi·ªÉm m√®o √ö b·ªã b·∫Øt kh√¥ng? H·ªç c√≥ th·ªÉ cung c·∫•p th√¥ng tin quan tr·ªçng.\n",
      "\n",
      "3. **B·∫´y**: B·∫°n c√≥ bi·∫øt lo·∫°i b·∫´y n√†o ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ b·∫Øt m√®o √ö kh√¥ng? ƒêi·ªÅu n√†y c√≥ th·ªÉ gi√∫p x√°c ƒë·ªãnh ai c√≥ th·ªÉ ƒë√£ ƒë·∫∑t b·∫´y.\n",
      "\n",
      "4. **ƒê·ªông c∆°**: C√≥ ai trong khu v·ª±c c√≥ ƒë·ªông c∆° ƒë·ªÉ b·∫Øt m√®o √ö kh√¥ng? C√≥ th·ªÉ l√† m·ªôt ng∆∞·ªùi h√†ng x√≥m, m·ªôt ng∆∞·ªùi y√™u ƒë·ªông v·∫≠t, ho·∫∑c ai ƒë√≥ c√≥ m√¢u thu·∫´n v·ªõi b·∫°n.\n",
      "\n",
      "5. **D·∫•u v·∫øt**: C√≥ d·∫•u v·∫øt n√†o xung quanh khu v·ª±c m√† m√®o √ö b·ªã b·∫Øt kh√¥ng? V√≠ d·ª• nh∆∞ d·∫•u ch√¢n, ƒë·ªì v·∫≠t l·∫°, ho·∫∑c b·∫•t k·ª≥ ƒëi·ªÅu g√¨ c√≥ th·ªÉ gi√∫p x√°c ƒë·ªãnh th·ªß ph·∫°m.\n",
      "\n",
      "N·∫øu b·∫°n c√≥ th√™m th√¥ng tin n√†o, h√£y chia s·∫ª ƒë·ªÉ ch√∫ng ta c√≥ th·ªÉ c√πng nhau t√¨m ra ai ƒë√£ b·∫Øt m√®o √ö!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode= \"context\",\n",
    "    system_prompt=(\"B·∫°n l√† m·ªôt nh√† th√°m t·ª≠, \"\n",
    "                   \"h√£y gi√∫p t√¥i t√¨m ra ai ƒë√£ b·∫Øt m√®o √ö!\")\n",
    ")\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CondenseQuestionChatEngine - Ch·∫ø ƒë·ªô c√¢u h·ªèi c√¥ ƒë·ªçng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: Xin ch√†o! B·∫°n c√≥ mu·ªën chia s·∫ª th√™m v·ªÅ M√®o √ö kh√¥ng?\n",
      "\n",
      "Assistant: M√®o √ö c·ªßa b·∫°n ƒëang ng·ªß b√™n c·ª≠a s·ªï.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode= \"condense_question\"\n",
    ")\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condense Plus Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "text = \"ƒê√™m qua ƒëi ch∆°i v·ªÅ mu·ªôn, m√®o √ö b·ªã s·∫≠p b·∫´y!\"\n",
    "doc = Document(text=text)\n",
    "index = VectorStoreIndex.from_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Entering Chat REPL =====\n",
      "Type \"exit\" to exit.\n",
      "\n",
      "Assistant: Xin ch√†o! B·∫°n c√≥ kh·ªèe kh√¥ng? N·∫øu b·∫°n mu·ªën chia s·∫ª ƒëi·ªÅu g√¨ v·ªÅ m√®o c·ªßa b·∫°n ho·∫∑c b·∫•t k·ª≥ ƒëi·ªÅu g√¨ kh√°c, m√¨nh r·∫•t s·∫µn l√≤ng l·∫Øng nghe! M√®o √ö c·ªßa b·∫°n ƒëang l√†m g√¨?\n",
      "\n",
      "Assistant: M√®o th∆∞·ªùng kh√¥ng k√™u \"g√¢u g√¢u\" nh∆∞ ch√≥, m√† ch√∫ng th∆∞·ªùng ph√°t ra √¢m thanh nh∆∞ \"meo meo\". C√≥ th·ªÉ b·∫°n ƒëang nh·∫ßm l·∫´n gi·ªØa ti·∫øng k√™u c·ªßa m√®o v√† ch√≥. M√®o √ö c·ªßa b·∫°n c√≥ th·ªÉ c√≥ nh·ªØng √¢m thanh ƒë·∫∑c tr∆∞ng ri√™ng, nh∆∞ng ch·∫Øc ch·∫Øn l√† kh√¥ng ph·∫£i \"g√¢u g√¢u\". N·∫øu b·∫°n c√≥ b·∫•t k·ª≥ c√¢u h·ªèi n√†o kh√°c v·ªÅ m√®o, m√¨nh r·∫•t vui l√≤ng gi√∫p ƒë·ª°!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode= \"condense_plus_context\"\n",
    ")\n",
    "chat_engine.chat_repl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xin l·ªói, nh∆∞ng t√¥i kh√¥ng th·ªÉ ki·ªÉm tra th·ªùi gian hi·ªán t·∫°i. B·∫°n c√≥ th·ªÉ xem ƒë·ªìng h·ªì ho·∫∑c thi·∫øt b·ªã c·ªßa m√¨nh ƒë·ªÉ bi·∫øt gi·ªù nh√©!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults()\n",
    "respone = chat_engine.chat(\"Em ∆°i m·∫•y gi·ªù r·ªìi?\")\n",
    "print(respone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_date_time()->str:\n",
    "    \"\"\"Get current date and time\n",
    "\n",
    "    Returns:\n",
    "        str: Current date and time\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "get_date_time_tool = FunctionTool.from_defaults(fn=get_date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    tools = [get_date_time_tool],\n",
    ")\n",
    "response = agent.chat(\"Em ∆°i m·∫•y gi·ªù r·ªìi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B√¢y gi·ªù l√† 22 gi·ªù 32 ph√∫t.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Engine Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B√¢y gi·ªù l√† 23 gi·ªù 08 ph√∫t.\n",
      "M√®o √ö c·ªßa anh ƒë√£ b·ªã s·∫≠p b·∫´y khi ƒëi ch∆°i v·ªÅ mu·ªôn. Anh c√≥ th·ªÉ t√¨m ki·∫øm xung quanh khu v·ª±c ƒë√≥ ƒë·ªÉ xem c√≥ th·ªÉ t√¨m th·∫•y m√®o √ö kh√¥ng.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "\n",
    "text = \"ƒê√™m qua ƒëi ch∆°i v·ªÅ mu·ªôn, m√®o √ö b·ªã s·∫≠p b·∫´y!\"\n",
    "doc = Document(text=text)\n",
    "index = VectorStoreIndex.from_documents([doc])\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "query_tool = QueryEngineTool.from_defaults(query_engine=query_engine, \n",
    "                                           description=\"C√¥ng c·ª• t√¨m ki·∫øm th√¥ng tin v·ªÅ m√®o √ö\")\n",
    "agent = OpenAIAgent.from_tools(tools=[query_tool, get_date_time_tool])\n",
    "\n",
    "respone_1 = agent.chat(\"Em ∆°i m·∫•y gi·ªù r·ªìi?\")\n",
    "respone_2 = agent.chat(\"Em c√≥ bi·∫øt m√®o √ö c·ªßa anh ·ªü ƒë√¢u kh√¥ng?\")\n",
    "\n",
    "print(respone_1)\n",
    "print(respone_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi·ªán t·∫°i l√† 23:42:51. \n",
      "\n",
      "M√®o √ö l√† m·ªôt con m√®o ƒë√£ b·ªã s·∫≠p b·∫´y khi ƒëi ch∆°i v·ªÅ mu·ªôn.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "\n",
    "text = \"ƒê√™m qua ƒëi ch∆°i v·ªÅ mu·ªôn, m√®o √ö b·ªã s·∫≠p b·∫´y!\"\n",
    "doc = Document(text=text)\n",
    "index = VectorStoreIndex.from_documents([doc])\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "query_tool = QueryEngineTool.from_defaults(query_engine=query_engine, \n",
    "                                           description=\"C√¥ng c·ª• t√¨m ki·∫øm th√¥ng tin v·ªÅ m√®o √ö\")\n",
    "agent = OpenAIAgent.from_tools(tools=[query_tool, get_date_time_tool])\n",
    "\n",
    "respone_1 = agent.chat(\"Em ∆°i m·∫•y gi·ªù r·ªìi? M√®o √∫ c·ªßa anh ƒë√¢u?\")\n",
    "\n",
    "print(respone_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Thu ∆°i, h√£y nh√¢n 5 v·ªõi 3 gi√∫p t·ªõ nh√©. M√† c·∫≠u bi·∫øt m√®o √ö c·ªßa t·ªõ ƒë√£ th·∫ø n√†o kh√¥ng?\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\": 5, \"b\": 3}\n",
      "Got output: 15\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"m√®o √ö\"}\n",
      "Got output: M√®o √ö l√† m·ªôt con m√®o ƒë√£ tr·ªü v·ªÅ v√†o s√°ng ng√†y 4 th√°ng 9 nƒÉm 2024, d∆∞·ªõi c∆°n m∆∞a t·∫ßm t√£.\n",
      "========================\n",
      "\n",
      "K·∫øt qu·∫£ c·ªßa ph√©p nh√¢n 5 v·ªõi 3 l√† 15. \n",
      "\n",
      "C√≤n v·ªÅ m√®o √ö c·ªßa c·∫≠u, n√≥ ƒë√£ tr·ªü v·ªÅ v√†o s√°ng ng√†y 4 th√°ng 9 nƒÉm 2024, d∆∞·ªõi c∆°n m∆∞a t·∫ßm t√£. Hy v·ªçng m√®o √ö c·ªßa c·∫≠u ƒë√£ an to√†n v√† kh·ªèe m·∫°nh!\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Tr·∫£ v·ªÅ k·∫øt qu·∫£ ph√©p nh√¢n a v·ªõi b\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def save_result(result: int) -> str:\n",
    "    \"\"\"L∆∞u k·∫øt qu·∫£ v√†o file result.txt\"\"\"\n",
    "    with open(\"result.txt\", \"w\") as f:\n",
    "        f.write(str(result))\n",
    "    return \"Result saved!\"\n",
    "\n",
    "text = \"S√°ng ng√†y 4 th√°ng 9 nƒÉm 2024, m√®o √ö tr·ªü v·ªÅ d∆∞·ªõi c∆°n m∆∞a t·∫ßm t√£.\"\n",
    "doc = Document(text=text)\n",
    "index = VectorStoreIndex.from_documents([doc])\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(query_engine=query_engine, \n",
    "                                           description=\"C√¥ng c·ª• t√¨m ki·∫øm th√¥ng tin v·ªÅ m√®o √ö\")\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "save_result_tool = FunctionTool.from_defaults(fn=save_result)\n",
    "\n",
    "openai_agent = OpenAIAgent.from_tools(\n",
    "    tools=[query_tool, multiply_tool, save_result_tool],\n",
    "    system_prompt=\"B·∫°n l√† Thu, ng∆∞·ªùi b·∫°n tri k·ª∑, hi·ªÉu chuy·ªán v√† th√†nh th·∫≠t c·ªßa t√¥i.\",\n",
    "    verbose=True\n",
    ")\n",
    "response = openai_agent.chat(\"Thu ∆°i, h√£y nh√¢n 5 v·ªõi 3 gi√∫p t·ªõ nh√©. M√† c·∫≠u bi·∫øt m√®o √ö c·ªßa t·ªõ ƒë√£ th·∫ø n√†o kh√¥ng?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## React Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 47f4f1b4-0297-41a9-9154-3346569e5d9a. Step input: Thu ∆°i, h√£y nh√¢n 5 v·ªõi 3 gi√∫p t·ªõ nh√©. M√† c·∫≠u bi·∫øt m√®o √ö c·ªßa t·ªõ ƒë√£ th·∫ø n√†o kh√¥ng?\n",
      "\u001b[1;3;38;5;200mThought: Ng∆∞·ªùi d√πng ƒëang y√™u c·∫ßu t√¥i th·ª±c hi·ªán ph√©p nh√¢n 5 v·ªõi 3 v√† c≈©ng h·ªèi v·ªÅ m√®o √ö c·ªßa h·ªç. T√¥i s·∫Ω b·∫Øt ƒë·∫ßu b·∫±ng c√°ch th·ª±c hi·ªán ph√©p nh√¢n tr∆∞·ªõc.\n",
      "Action: multiply\n",
      "Action Input: {'a': 5, 'b': 3}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 15\n",
      "\u001b[0m> Running step 4b73701e-9f2c-4b91-93fc-cbf34466808e. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: T√¥i ƒë√£ th·ª±c hi·ªán ph√©p nh√¢n v√† k·∫øt qu·∫£ l√† 15. B√¢y gi·ªù t√¥i s·∫Ω t√¨m ki·∫øm th√¥ng tin v·ªÅ m√®o √ö c·ªßa ng∆∞·ªùi d√πng.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'm√®o √ö'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: M√®o √ö l√† m·ªôt con m√®o ƒë√£ tr·ªü v·ªÅ v√†o s√°ng ng√†y 4 th√°ng 9 nƒÉm 2024, d∆∞·ªõi c∆°n m∆∞a t·∫ßm t√£.\n",
      "\u001b[0m> Running step ef519902-035a-4260-98d0-96bf691aa077. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: T√¥i ƒë√£ c√≥ th√¥ng tin v·ªÅ m√®o √ö c·ªßa ng∆∞·ªùi d√πng. B√¢y gi·ªù t√¥i c√≥ th·ªÉ tr·∫£ l·ªùi c·∫£ hai y√™u c·∫ßu.\n",
      "Answer: K·∫øt qu·∫£ ph√©p nh√¢n 5 v·ªõi 3 l√† 15. M√®o √ö c·ªßa b·∫°n ƒë√£ tr·ªü v·ªÅ v√†o s√°ng ng√†y 4 th√°ng 9 nƒÉm 2024, d∆∞·ªõi c∆°n m∆∞a t·∫ßm t√£.\n",
      "\u001b[0mK·∫øt qu·∫£ ph√©p nh√¢n 5 v·ªõi 3 l√† 15. M√®o √ö c·ªßa b·∫°n ƒë√£ tr·ªü v·ªÅ v√†o s√°ng ng√†y 4 th√°ng 9 nƒÉm 2024, d∆∞·ªõi c∆°n m∆∞a t·∫ßm t√£.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.react import ReActAgent\n",
    "\n",
    "react_agent = ReActAgent.from_tools(\n",
    "    tools=[query_tool, multiply_tool, save_result_tool],\n",
    "    system_prompt=\"B·∫°n l√† Thu, ng∆∞·ªùi b·∫°n tri k·ª∑, hi·ªÉu chuy·ªán v√† th√†nh th·∫≠t c·ªßa t√¥i.\",\n",
    "    verbose=True\n",
    ")\n",
    "response = react_agent.chat(\"Thu ∆°i, h√£y nh√¢n 5 v·ªõi 3 gi√∫p t·ªõ nh√©. M√† c·∫≠u bi·∫øt m√®o √ö c·ªßa t·ªõ ƒë√£ th·∫ø n√†o kh√¥ng?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pits_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
