{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index==0.11.4\n",
    "# !pip install PyYAML\n",
    "# !pip install plotly==5.24.0\n",
    "# !pip install docx2txt==0.8\n",
    "# !pip install chromadb==0.5.5\n",
    "# !pip install llama-index-vector-stores-chroma==0.2.0\n",
    "# !pip install llama-index-extractors-entity==0.2.0\n",
    "# !pip install llama-index-readers-web==0.2.2\n",
    "# !pip install transformers==4.40.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Index cơ bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta sẽ tìm hiểu kiến trúc của một hệ thống RAG cơ bản được xây dựng bởi Llama index sẽ gồm các thành phần nào. Bạn có thể tạo một ứng dụng RAG cơ bản sau khi đọc xong phần này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents\n",
    "\n",
    "Một thành phần không thể thiếu trong các ứng dụng RAG là dữ liệu, chúng ta có nhiều loại dữ liệu khác nhau như: pdf, docx, csv, pptx, html, database... mỗi loại khác nhau không tuân theo một quy chuẩn chung nào. Chính vì vậy, Documents trong Llama Index được tạo ra với vai trò như một cái khuôn để chứa và đưa các loại dữ liệu về một loại cấu trúc chung.\n",
    "\n",
    "Điểm đặc biệt trong cấu trúc của Documents là nó có thể chứa thêm các thông tin bổ xung về dữ liệu trong đó, thành phần này gọi là siêu dữ liệu(metadata). Hiểu đơn giản thì nó là các thông tin bổ xung mà chúng ta cung cấp thêm cho dữ liệu như mô tả, tóm tắt, tiêu đề... những thông tin này nhằm mục đích hỗ trợ quá trình tìm kiếm hiệu quả hơn.\n",
    "\n",
    "Để tạo Document từ dữ liệu, cú pháp sử dụng như sau: `Document(text, metadata, id_)`\n",
    "Trong đó:\n",
    "\n",
    "    * text: dữ liệu đầu vào dạng text\n",
    "\n",
    "    * metadata: siêu dữ liệu, thường có định dạng dictionary, thường được tạo tự động bởi thư viện.\n",
    "\n",
    "    * id_: Chỉ số của đối tượng Document này, nó thường được tạo một cách tự động\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 1\n",
      "Text: AI VIET NAM\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "text = \"AI VIET NAM\"\n",
    "doc = Document(\n",
    "        text=text,\n",
    "        metadata={\"fb\": \"fb/aivietnam.edu.vn\"},\n",
    "        id_=\"1\"\n",
    "        )\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id_', '1')\n",
      "('embedding', None)\n",
      "('metadata', {'fb': 'fb/aivietnam.edu.vn'})\n",
      "('excluded_embed_metadata_keys', [])\n",
      "('excluded_llm_metadata_keys', [])\n",
      "('relationships', {})\n",
      "('text', 'AI VIET NAM')\n",
      "('mimetype', 'text/plain')\n",
      "('start_char_idx', None)\n",
      "('end_char_idx', None)\n",
      "('text_template', '{metadata_str}\\n\\n{content}')\n",
      "('metadata_template', '{key}: {value}')\n",
      "('metadata_seperator', '\\n')\n"
     ]
    }
   ],
   "source": [
    "for i in doc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ví dụ trên, chúng ta tạo một đối tượng Document với đầu vào là text-một chuỗi string, và siêu dữ liệu metadata là một dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes\n",
    "\n",
    "Việc tạo Documents khá đơn giản, tuy nhiên dữ liệu trong document vẫn là dữ liệu thô, vậy làm sao để  dữ liệu thô có thể chuyển đổi thành định dạng mà LLM có thể xử lý và suy luận hiệu quả. Giải pháp chính là Nodes, nó là những nội dung nhỏ hơn được trích xuất từ tài liệu, mục đích để chia tài liệu thành những phần nhỏ hơn để dễ quản lí.\n",
    "\n",
    "Nodes tránh được tình trạng vượt quá giới hạn của prompt mà mô hình cho phép. Ví dụ khi có 1 cuốn ebook 100 trang chúng ta không dùng toàn bộ dữ liệu trong đó trực tiếp vào prompt để LLM xử lý, vì nó sẽ vượt quá giới hạn đầu vào của mô hình. Hơn nữa cách này có nhược điểm là chi phí xử lí cao, chúng ta phải trả nhiều tiền hơn cho việc sử dụng API, độ chính xác của câu trả lời cũng không đảm bảo vì khi đầu vào là một văn bản dài như vậy, prompt của chúng ta không tập trung vào thông tin cụ thể nào dẫn đến mô hình không hiểu và đưa ra câu trả lời chính xác. Ngoài ra giữa các nodes chúng ta có thể thiết lập mối quan hệ giữa chúng.\n",
    "\n",
    "Trong Llamaindex, chúng ta có thể tạo nodes cho dữ liệu văn bản bằng cách sử dụng lớp TextNode. Cú pháp sử dụng:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 84f66afa-d8b1-4edf-9658-d3e44a746fab\n",
      "Text: AI\n",
      "Node ID: 7edd07b6-27dc-4ba4-9a84-a0b5e9ea200b\n",
      "Text: VIET NAM\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "text = \"AI VIET NAM\"\n",
    "doc = Document(\n",
    "        text=text\n",
    "        )\n",
    "\n",
    "node1 = TextNode(text=doc.text[:2])\n",
    "node2 = TextNode(text=doc.text[3:])\n",
    "\n",
    "print(node1)\n",
    "print(node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể tạo các node tự động bằng cách sử dụng TokenTextSplitter, cú pháp sử dụng:\n",
    "\n",
    "```Python\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "TokenTextSplitter(\n",
    "    chunk_size,\n",
    "    chunk_overlap,\n",
    "    separator,\n",
    ")\n",
    "```\n",
    "\n",
    "Trong đó: \n",
    "\n",
    "    - chunk_size: Kích thước của mỗi đoạn văn bản (chunk). Đây là số lượng token (từ hoặc ký tự) trong mỗi đoạn.\n",
    "\n",
    "    - chunk_overlap: Số lượng token sẽ bị trùng lặp giữa các đoạn liên tiếp. Điều này có nghĩa là một phần của đoạn trước sẽ được lặp lại ở đoạn sau.\n",
    "    \n",
    "    - separator: Ký tự hoặc chuỗi ký tự được sử dụng để phân tách các đoạn văn bản. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (2) is close to chunk size (20). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Node ID: 38081e25-27e8-4152-8da3-47627107ae55\n",
      "Text: Hôm nay trời nắng, tôi đi ăn\n",
      "Node ID: d5c34421-6eee-49b3-a7a4-b79de4afeb0d\n",
      "Text: đi ăn kem, lạnh buốt cả răng!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "text = \"Hôm nay trời nắng, tôi đi ăn kem, lạnh buốt cả răng!\"\n",
    "doc = Document(text=text)\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5,\n",
    "    separator= \" \"\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents([doc])\n",
    "\n",
    "for node in nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ví dụ trên, chúng ta gọi lớp TokenTextSplitter từ module node_parser trong gói llama_index.core. Tiếp theo, chúng ta tạo đối tượng doc từ text, sau đó \n",
    "tạo đối tượng splitter từ lớp TokenTextSplitter với các thuộc tính chunk_size, chunk_overlap, separator. Để tạo node, chúng ta gọi đến phương thức get_nodes_from_documents từ đối tượng splitter vừa tạo. Vậy là các node của chúng ta đã được tạo tự động. Một cảnh báo xuất hiện nói rằng chúng ta sẽ gặp vấn đề khi mà chunk_size của chúng ta quá nhỏ, nhỏ hơn hoặc chênh lệch không quá nhiều so với kích cỡ của metadata. Vì khi đó metadata sẽ chiếm phần lớn lượng dữ liệu của node trong khi dữ liệu thực tế  thì rất ít.\n",
    "\n",
    "Việc tạo node không chỉ dừng lại ở việc chia nhỏ document, chúng ta có thể thiết lập mối quan hệ giữa các node, ngoài ra còn có một số phương pháp biến đổi node nâng cao hơn mà chúng ta sẽ tìm hiểu ở phần sau. Dưới đây là cách mà chúng ta tạo mối quan hệ giữa hai node, có 5 mối quan hệ giữa các node là:\n",
    "Trong hệ thống phân chia văn bản thành các đoạn nhỏ (nodes) như được sử dụng trong thư viện `llama_index`, các mối quan hệ giữa các nodes có thể được mô tả như sau:\n",
    "\n",
    "1. **SOURCE**: \n",
    "   - Node này là tài liệu gốc (source document). Đây là toàn bộ văn bản ban đầu mà từ đó các nodes khác được tách ra.\n",
    "   - Ví dụ: Nếu văn bản gốc là \"Hôm nay trời nắng, tôi đi ăn kem, lạnh buốt cả răng!\", thì node `SOURCE` sẽ chứa toàn bộ văn bản này.\n",
    "\n",
    "2. **PREVIOUS**: \n",
    "   - Node này là node trước đó trong tài liệu. Nó đại diện cho đoạn văn bản ngay trước node hiện tại.\n",
    "   - Ví dụ: Nếu có hai nodes liên tiếp, node thứ hai sẽ có một liên kết `PREVIOUS` trỏ đến node thứ nhất.\n",
    "\n",
    "3. **NEXT**: \n",
    "   - Node này là node kế tiếp trong tài liệu. Nó đại diện cho đoạn văn bản ngay sau node hiện tại.\n",
    "   - Ví dụ: Nếu có hai nodes liên tiếp, node thứ nhất sẽ có một liên kết `NEXT` trỏ đến node thứ hai.\n",
    "\n",
    "4. **PARENT**: \n",
    "   - Node này là node cha trong tài liệu. Nó đại diện cho một đoạn văn bản lớn hơn chứa node hiện tại.\n",
    "   - Ví dụ: Nếu một tài liệu lớn được chia thành các đoạn nhỏ hơn và mỗi đoạn nhỏ này lại được chia thành các đoạn nhỏ hơn nữa, thì một node có thể có một node cha chứa nó.\n",
    "\n",
    "5. **CHILD**: \n",
    "   - Node này là node con trong tài liệu. Nó đại diện cho một đoạn văn bản nhỏ hơn nằm trong node hiện tại.\n",
    "   - Ví dụ: Nếu một đoạn văn bản lớn được chia thành các đoạn nhỏ hơn, thì mỗi đoạn nhỏ sẽ là một node con của node lớn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID: 2fa6a6a0-3911-4743-aa53-a4e7032c5607\n",
      "Text: Hôm nay trời nắng, tôi đi ăn kem, lạnh buốt cả răng!\n",
      "Node ID: 38081e25-27e8-4152-8da3-47627107ae55\n",
      "Text: Hôm nay trời nắng, tôi đi ăn\n",
      "Node ID: d5c34421-6eee-49b3-a7a4-b79de4afeb0d\n",
      "Text: đi ăn kem, lạnh buốt cả răng!\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "print(nodes[0])\n",
    "print(nodes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='38081e25-27e8-4152-8da3-47627107ae55', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2fa6a6a0-3911-4743-aa53-a4e7032c5607', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='0af2325bc3976ef85024e35769344ac7138fe4c21700c2bf55d2109039882bb4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d5c34421-6eee-49b3-a7a4-b79de4afeb0d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='524e34237455ee85fecf2724237a8e825e136e9dff359bbde44c4c3a656c1d95')}, text='Hôm nay trời nắng, tôi đi ăn', mimetype='text/plain', start_char_idx=0, end_char_idx=28, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='d5c34421-6eee-49b3-a7a4-b79de4afeb0d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2fa6a6a0-3911-4743-aa53-a4e7032c5607', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='0af2325bc3976ef85024e35769344ac7138fe4c21700c2bf55d2109039882bb4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='38081e25-27e8-4152-8da3-47627107ae55', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='edcd2c00a132ff9c5b9c159f7b6bcb26b00e8fc16d6df454d4f1c3639e01c294')}, text='đi ăn kem, lạnh buốt cả răng!', mimetype='text/plain', start_char_idx=23, end_char_idx=52, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2fa6a6a0-3911-4743-aa53-a4e7032c5607', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='0af2325bc3976ef85024e35769344ac7138fe4c21700c2bf55d2109039882bb4'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d5c34421-6eee-49b3-a7a4-b79de4afeb0d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='524e34237455ee85fecf2724237a8e825e136e9dff359bbde44c4c3a656c1d95')}\n",
      "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2fa6a6a0-3911-4743-aa53-a4e7032c5607', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='0af2325bc3976ef85024e35769344ac7138fe4c21700c2bf55d2109039882bb4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='38081e25-27e8-4152-8da3-47627107ae55', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='edcd2c00a132ff9c5b9c159f7b6bcb26b00e8fc16d6df454d4f1c3639e01c294')}\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].relationships)\n",
    "print(nodes[1].relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phần thiết lập mối quan hệ giữa các node các node này thường sẽ được tạo tự động, các bạn có thể thấy trong ví dụ trên, các mối quan hệ đã được tạo tự động. Ta có thể thấy node 1 có node SOURCE là doc, có mối quan hệ NEXT với node 2. Tương tự thì node 2 có mối quan hệ PREVIOUS với node 1. Nói chung  chúng ta chỉ cần biết là các mối quan hệ này rất quan trọng, giúp việc truy vấn chính xác hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "Sau khi bạn nhập dữ liệu vào hệ thống, LlamaIndex sẽ giúp bạn lập chỉ mục dữ liệu vào một cấu trúc dễ dàng truy xuất. Quá trình này thường bao gồm việc tạo ra các vector embeddings và lưu trữ chúng trong một cơ sở dữ liệu chuyên biệt gọi là vector store. Việc lập chỉ mục giúp tổ chức dữ liệu sao cho dễ dàng tìm kiếm và truy xuất sau này.\n",
    "\n",
    "Llama Index hỗ trợ nhiều loại index khác nhau như SummaryIndex, VectorStoreIndex, TreeIndex, KnowledgeGraphIndex, tùy vào từng trường hợp mà chúng ta sẽ chọn loại phù hợp. Nhìn chung, các loại index này đều thực hiện các chức năng tạo index, thêm node mới, và truy vấn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (2) is close to chunk size (20). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "index 1 IndexList(index_id='e30eaa05-fa23-4691-aee0-202137c60a79', summary=None, nodes=['86f99abc-3dac-4f52-9e6f-bfd56bc9557e', 'b987d5bc-0e51-4d60-a03d-2ad0f5c30cd0', '7374ac68-e3ed-4283-b3b4-5b72c42bb1c5'])\n",
      "index 2 IndexList(index_id='9f45e5a7-0942-4d7f-8dac-d5037c02313c', summary=None, nodes=['e74a7bee-125e-47c0-85fc-68922a6c39d8'])\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, SummaryIndex\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "text = \"Con mèo ú nằm ườn bên cửa sổ. Tôi muốn mình cũng được như thế.\"\n",
    "doc = Document(\n",
    "    text=text\n",
    "    )\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5,\n",
    "    separator= \" \"\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents([doc])\n",
    "index = SummaryIndex(nodes)\n",
    "index2 = SummaryIndex.from_documents([doc])\n",
    "\n",
    "print(\"index 1\", index.index_struct)\n",
    "print(\"index 2\", index2.index_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2 = SummaryIndex.from_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexList(index_id='e30eaa05-fa23-4691-aee0-202137c60a79', summary=None, nodes=['86f99abc-3dac-4f52-9e6f-bfd56bc9557e', 'b987d5bc-0e51-4d60-a03d-2ad0f5c30cd0', '7374ac68-e3ed-4283-b3b4-5b72c42bb1c5'])\n"
     ]
    }
   ],
   "source": [
    "print(index.index_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ví dụ trên chúng ta taọ index từ nodes sử dụng SummayIndex, chúng ta có thể xem cấu trúc của index qua index_struct. Vậy là index đã sẵn sàng để truy xuất, từ đầu đến giờ chúng ta tạo Doc, tạo Node, tạo Index cũng chỉ nhằm mục đích để truy xuất thông tin từ dữ liệu thôi phải không nào? Để truy xuất dữ liệu trong index, Llama Index hỗ trợ nhiều công cụ truy xuất khác nhau, nhưng đơn giản nhất ở đây là mình tạo đối tượng query_engine từ index.as_query_engine(), tức là chúng ta tạo một công cụ hỏi đáp từ index. Việc sử dụng cũng rất đơn giản chỉ cần gọi phương thức query_engine.query() cùng với tham số là nội dung câu hỏi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/aio_mental_health/lib/python3.11/site-packages/llama_index/core/llms/utils.py:42\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     41\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aio_mental_health/lib/python3.11/site-packages/llama_index/llms/openai/utils.py:410\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m respone \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmèo ú đang làm gì?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(respone)\n",
      "File \u001b[0;32m~/miniconda3/envs/aio_mental_health/lib/python3.11/site-packages/llama_index/core/indices/base.py:377\u001b[0m, in \u001b[0;36mBaseIndex.as_query_engine\u001b[0;34m(self, llm, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    370\u001b[0m     RetrieverQueryEngine,\n\u001b[1;32m    371\u001b[0m )\n\u001b[1;32m    373\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_retriever(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m llm \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    375\u001b[0m     resolve_llm(llm, callback_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager)\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RetrieverQueryEngine\u001b[38;5;241m.\u001b[39mfrom_args(\n\u001b[1;32m    381\u001b[0m     retriever,\n\u001b[1;32m    382\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    384\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aio_mental_health/lib/python3.11/site-packages/llama_index/core/settings.py:36\u001b[0m, in \u001b[0;36m_Settings.llm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the LLM.\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[0;32m~/miniconda3/envs/aio_mental_health/lib/python3.11/site-packages/llama_index/core/llms/utils.py:49\u001b[0m, in \u001b[0;36mresolve_llm\u001b[0;34m(llm, callback_manager)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-llms-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-llms-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m         )\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     60\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "respone = query_engine.query(\"mèo ú đang làm gì?\")\n",
    "print(respone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong đoạn code trên, chúng ta gặp lỗi vì chưa thiết lập khóa API của OPEN AI nên không thể sử dụng query_engine. Bây giờ chúng ta hãy thêm khóa để sửa lỗi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-proj-your-api-key\"\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong đoạn code trên, chúng ta thiết lập api_key từ open api, ngoài ra chúng ta thiết lập mô hình LLM mà chúng ta sử dụng là gpt-4o-mini, với temperature là 0.2. Ở đây, temperature có giá trị từ 0 đến 2, giá trị càng cao thì câu trả lời càng sáng tạo, ngẫu nhiên. Khi giá trị thấp hơn hoặc bằng 0 thì kết quả cho ra sẽ cố định hơn, tức là chúng ta hỏi một câu hỏi nhiều lần thì kết quả trả lời đều giống nhau. Nói chung tùy mục đích sử dụng mà ta sẽ điều chỉnh con số hợp lý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mèo ú đang nằm ườn bên cửa sổ.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "respone = query_engine.query(\"mèo ú đang làm gì?\")\n",
    "print(respone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy là chúng ta đã tìm hiểu về các khái niệm cơ bản trong llama index và xây dựng thành công mô trình truy vấn RAG đơn giản. Dưới đây là tóm tắt các bước xây dựng mô hình RAG cơ bản:\n",
    "\n",
    "1. Tải dữ liệu dưới dạng Documents\n",
    "2. Phân tích tài liệu thành các nút mạch lạc\n",
    "3. Xây dựng chỉ mục được tối ưu hóa từ Nút\n",
    "4. Chạy truy vấn trên chỉ mục để truy xuất các Nút có liên quan\n",
    "5. Tổng hợp phản hồi cuối cùng"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pits_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
