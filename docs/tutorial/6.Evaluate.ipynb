{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index==0.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-your-api-key\"\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (2) is close to chunk size (20). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "text = \"Hôm nay trời nắng, tôi đi ăn kem, lạnh buốt cả răng!\"\n",
    "doc = Document(text=text)\n",
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5,\n",
    "    separator= \" \"\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents([doc])\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    BatchEvalRunner,\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "import asyncio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "\n",
    "dataset_generator2 =  RagDatasetGenerator(nodes, num_questions_per_chunk=1)\n",
    "eval_questions2 = dataset_generator2.generate_questions_from_nodes()\n",
    "df = eval_questions2.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hôm nay thời tiết như thế nào và bạn đã làm gì...</td>\n",
       "      <td>[Hôm nay trời nắng, tôi đi ăn]</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>ai (gpt-4o-mini)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What sensory experience is described in the ph...</td>\n",
       "      <td>[đi ăn kem, lạnh buốt cả răng!]</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>ai (gpt-4o-mini)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Hôm nay thời tiết như thế nào và bạn đã làm gì...   \n",
       "1  What sensory experience is described in the ph...   \n",
       "\n",
       "                reference_contexts reference_answer reference_answer_by  \\\n",
       "0   [Hôm nay trời nắng, tôi đi ăn]                                 None   \n",
       "1  [đi ăn kem, lạnh buốt cả răng!]                                 None   \n",
       "\n",
       "           query_by  \n",
       "0  ai (gpt-4o-mini)  \n",
       "1  ai (gpt-4o-mini)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the evaluators\n",
    "correctness_evaluator = CorrectnessEvaluator() # Useful for measuring if the response is correct against a reference answer\n",
    "faithfulness_evaluator = FaithfulnessEvaluator() # Useful for measuring if the response is hallucinated\n",
    "relevancy_evaluator = RelevancyEvaluator() # Useful for measuring if the query is actually answered by the response\n",
    "\n",
    "# Define an asynchronous function for evaluation\n",
    "async def evaluate_async():\n",
    "    # Initialize the BatchEvalRunner\n",
    "    runner = BatchEvalRunner(\n",
    "        {\n",
    "            \"correctness\": correctness_evaluator,\n",
    "            \"faithfulness\": faithfulness_evaluator,\n",
    "            \"relevancy\": relevancy_evaluator\n",
    "        },\n",
    "        show_progress = True\n",
    "    )\n",
    "\n",
    "    # Run the asynchronous evaluation\n",
    "    eval_result = await runner.aevaluate_queries(\n",
    "        query_engine = vector_index.as_query_engine(),\n",
    "        queries = [question for question in df['query']],\n",
    "    )\n",
    "\n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.91s/it]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "result = asyncio.run(evaluate_async())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correctness': [EvaluationResult(query='Hôm nay thời tiết như thế nào và bạn đã làm gì trong ngày hôm đó?', contexts=None, response='Hôm nay trời nắng và bạn đã đi ăn kem.', passing=True, feedback=\"The generated answer is relevant to the user query as it provides information about the weather and an activity done during the day. However, it lacks detail about the specific weather conditions and the user's activities, which could enhance the response.\", score=4.0, pairwise_source=None, invalid_result=False, invalid_reason=None),\n",
       "  EvaluationResult(query='What sensory experience is described in the phrase \"lạnh buốt cả răng\" in relation to eating ice cream?', contexts=None, response='The phrase \"lạnh buốt cả răng\" describes the sensation of extreme cold that affects the teeth when eating ice cream. It conveys a sharp, intense feeling of coldness that can be uncomfortable or even painful.', passing=True, feedback='The generated answer accurately describes the sensory experience related to the phrase \"lạnh buốt cả răng\" in the context of eating ice cream, capturing both the intensity and discomfort associated with the sensation.', score=5.0, pairwise_source=None, invalid_result=False, invalid_reason=None)],\n",
       " 'faithfulness': [EvaluationResult(query='Hôm nay thời tiết như thế nào và bạn đã làm gì trong ngày hôm đó?', contexts=['Hôm nay trời nắng, tôi đi ăn', 'đi ăn kem, lạnh buốt cả răng!'], response='Hôm nay trời nắng và bạn đã đi ăn kem.', passing=True, feedback='YES', score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None),\n",
       "  EvaluationResult(query='What sensory experience is described in the phrase \"lạnh buốt cả răng\" in relation to eating ice cream?', contexts=['đi ăn kem, lạnh buốt cả răng!', 'Hôm nay trời nắng, tôi đi ăn'], response='The phrase \"lạnh buốt cả răng\" describes the sensation of extreme cold that affects the teeth when eating ice cream. It conveys a sharp, intense feeling of coldness that can be uncomfortable or even painful.', passing=False, feedback='NO', score=0.0, pairwise_source=None, invalid_result=False, invalid_reason=None)],\n",
       " 'relevancy': [EvaluationResult(query='Hôm nay thời tiết như thế nào và bạn đã làm gì trong ngày hôm đó?', contexts=['Hôm nay trời nắng, tôi đi ăn', 'đi ăn kem, lạnh buốt cả răng!'], response='Hôm nay trời nắng và bạn đã đi ăn kem.', passing=True, feedback='YES', score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None),\n",
       "  EvaluationResult(query='What sensory experience is described in the phrase \"lạnh buốt cả răng\" in relation to eating ice cream?', contexts=['đi ăn kem, lạnh buốt cả răng!', 'Hôm nay trời nắng, tôi đi ăn'], response='The phrase \"lạnh buốt cả răng\" describes the sensation of extreme cold that affects the teeth when eating ice cream. It conveys a sharp, intense feeling of coldness that can be uncomfortable or even painful.', passing=True, feedback='YES', score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, question in enumerate(df['query']):\n",
    "    correctness_result = result['correctness'][i]\n",
    "    faithfulness_result = result['faithfulness'][i]\n",
    "    relevancy_result = result['relevancy'][i]\n",
    "    data.append({\n",
    "        'Query': question,\n",
    "        'Correctness response': correctness_result.response,\n",
    "        'Correctness passing': correctness_result.passing,\n",
    "        'Correctness feedback': correctness_result.feedback,\n",
    "        'Correctness score': correctness_result.score,\n",
    "        'Faithfulness response': faithfulness_result.response,\n",
    "        'Faithfulness passing': faithfulness_result.passing,\n",
    "        'Faithfulness feedback': faithfulness_result.feedback,\n",
    "        'Faithfulness score': faithfulness_result.score,\n",
    "        'Relevancy response': relevancy_result.response,\n",
    "        'Relevancy passing': relevancy_result.passing,\n",
    "        'Relevancy feedback': relevancy_result.feedback,\n",
    "        'Relevancy score': relevancy_result.score,\n",
    "    })\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df3 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Correctness response</th>\n",
       "      <th>Correctness passing</th>\n",
       "      <th>Correctness feedback</th>\n",
       "      <th>Correctness score</th>\n",
       "      <th>Faithfulness response</th>\n",
       "      <th>Faithfulness passing</th>\n",
       "      <th>Faithfulness feedback</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Relevancy response</th>\n",
       "      <th>Relevancy passing</th>\n",
       "      <th>Relevancy feedback</th>\n",
       "      <th>Relevancy score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hôm nay thời tiết như thế nào và bạn đã làm gì...</td>\n",
       "      <td>Hôm nay trời nắng và bạn đã đi ăn kem.</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer is relevant to the user q...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Hôm nay trời nắng và bạn đã đi ăn kem.</td>\n",
       "      <td>True</td>\n",
       "      <td>YES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hôm nay trời nắng và bạn đã đi ăn kem.</td>\n",
       "      <td>True</td>\n",
       "      <td>YES</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What sensory experience is described in the ph...</td>\n",
       "      <td>The phrase \"lạnh buốt cả răng\" describes the s...</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated answer accurately describes the ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The phrase \"lạnh buốt cả răng\" describes the s...</td>\n",
       "      <td>False</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The phrase \"lạnh buốt cả răng\" describes the s...</td>\n",
       "      <td>True</td>\n",
       "      <td>YES</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  Hôm nay thời tiết như thế nào và bạn đã làm gì...   \n",
       "1  What sensory experience is described in the ph...   \n",
       "\n",
       "                                Correctness response  Correctness passing  \\\n",
       "0             Hôm nay trời nắng và bạn đã đi ăn kem.                 True   \n",
       "1  The phrase \"lạnh buốt cả răng\" describes the s...                 True   \n",
       "\n",
       "                                Correctness feedback  Correctness score  \\\n",
       "0  The generated answer is relevant to the user q...                4.0   \n",
       "1  The generated answer accurately describes the ...                5.0   \n",
       "\n",
       "                               Faithfulness response  Faithfulness passing  \\\n",
       "0             Hôm nay trời nắng và bạn đã đi ăn kem.                  True   \n",
       "1  The phrase \"lạnh buốt cả răng\" describes the s...                 False   \n",
       "\n",
       "  Faithfulness feedback  Faithfulness score  \\\n",
       "0                   YES                 1.0   \n",
       "1                    NO                 0.0   \n",
       "\n",
       "                                  Relevancy response  Relevancy passing  \\\n",
       "0             Hôm nay trời nắng và bạn đã đi ăn kem.               True   \n",
       "1  The phrase \"lạnh buốt cả răng\" describes the s...               True   \n",
       "\n",
       "  Relevancy feedback  Relevancy score  \n",
       "0                YES              1.0  \n",
       "1                YES              1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness scores: 4.5\n",
      "Faithfulness scores: 0.5\n",
      "Relevancy scores: 1.0\n"
     ]
    }
   ],
   "source": [
    "correctness_scores = df3['Correctness score'].mean()\n",
    "Faithfulness_scores = df3['Faithfulness score'].mean()\n",
    "Relevancy_scores = df3['Relevancy score'].mean()\n",
    "print(f\"Correctness scores: {correctness_scores}\")\n",
    "print(f\"Faithfulness scores: {Faithfulness_scores}\")\n",
    "print(f\"Relevancy scores: {Relevancy_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from llama_index.core import Settings, Document, VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.evaluation import (\n",
    "    BatchEvalRunner,\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator\n",
    ")\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# Set up necessary settings for LLM\n",
    "def setup_openai(api_key: str, model: str = \"gpt-4o-mini\", temperature: float = 0.2):\n",
    "    openai.api_key = api_key\n",
    "    Settings.llm = OpenAI(model=model, temperature=temperature)\n",
    "\n",
    "# Split text into smaller chunks for processing\n",
    "def create_document_and_splitter(text: str, chunk_size: int = 20, chunk_overlap: int = 5, separator: str = \" \"):\n",
    "    doc = Document(text=text)\n",
    "    splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separator=separator\n",
    "    )\n",
    "    nodes = splitter.get_nodes_from_documents([doc])\n",
    "    return nodes\n",
    "\n",
    "# Create a vector store index and a query engine\n",
    "def create_vector_store_index(nodes):\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    return query_engine\n",
    "\n",
    "# Generate questions from nodes\n",
    "def generate_questions(nodes, num_questions_per_chunk: int = 1):\n",
    "    dataset_generator = RagDatasetGenerator(nodes, num_questions_per_chunk=num_questions_per_chunk)\n",
    "    eval_questions = dataset_generator.generate_questions_from_nodes()\n",
    "    return eval_questions.to_pandas()\n",
    "\n",
    "# Define asynchronous evaluation function\n",
    "async def evaluate_async(query_engine, df):\n",
    "    correctness_evaluator = CorrectnessEvaluator() # Evaluate correctness against a reference answer\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator() # Evaluate hallucination of the response\n",
    "    relevancy_evaluator = RelevancyEvaluator() # Evaluate if the response actually answers the query\n",
    "\n",
    "    # Initialize the BatchEvalRunner\n",
    "    runner = BatchEvalRunner(\n",
    "        {\n",
    "            \"correctness\": correctness_evaluator,\n",
    "            \"faithfulness\": faithfulness_evaluator,\n",
    "            \"relevancy\": relevancy_evaluator\n",
    "        },\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Run the asynchronous evaluation\n",
    "    eval_result = await runner.aevaluate_queries(\n",
    "        query_engine= query_engine,\n",
    "        queries=[question for question in df['query']],\n",
    "    )\n",
    "\n",
    "    return eval_result\n",
    "\n",
    "# Collect and aggregate the results\n",
    "def aggregate_results(df, eval_result):\n",
    "    data = []\n",
    "    for i, question in enumerate(df['query']):\n",
    "        correctness_result = eval_result['correctness'][i]\n",
    "        faithfulness_result = eval_result['faithfulness'][i]\n",
    "        relevancy_result = eval_result['relevancy'][i]\n",
    "        data.append({\n",
    "            'Query': question,\n",
    "            'Correctness response': correctness_result.response,\n",
    "            'Correctness passing': correctness_result.passing,\n",
    "            'Correctness feedback': correctness_result.feedback,\n",
    "            'Correctness score': correctness_result.score,\n",
    "            'Faithfulness response': faithfulness_result.response,\n",
    "            'Faithfulness passing': faithfulness_result.passing,\n",
    "            'Faithfulness feedback': faithfulness_result.feedback,\n",
    "            'Faithfulness score': faithfulness_result.score,\n",
    "            'Relevancy response': relevancy_result.response,\n",
    "            'Relevancy passing': relevancy_result.passing,\n",
    "            'Relevancy feedback': relevancy_result.feedback,\n",
    "            'Relevancy score': relevancy_result.score,\n",
    "        })\n",
    "\n",
    "    # Create a pandas DataFrame from the data\n",
    "    df_result = pd.DataFrame(data)\n",
    "    return df_result\n",
    "\n",
    "# Calculate and print average scores\n",
    "def print_average_scores(df):\n",
    "    correctness_scores = df['Correctness score'].mean()\n",
    "    faithfulness_scores = df['Faithfulness score'].mean()\n",
    "    relevancy_scores = df['Relevancy score'].mean()\n",
    "    print(f\"Correctness scores: {correctness_scores}\")\n",
    "    print(f\"Faithfulness scores: {faithfulness_scores}\")\n",
    "    print(f\"Relevancy scores: {relevancy_scores}\")\n",
    "\n",
    "# Main function to execute the steps\n",
    "def main():\n",
    "    # Apply nested asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Setup OpenAI\n",
    "    setup_openai(api_key=\"sk-\")\n",
    "\n",
    "    # Create document and split into nodes\n",
    "    text = \"Hôm nay trời nắng, tôi đi ăn kem, lạnh buốt cả răng!\"\n",
    "    nodes = create_document_and_splitter(text)\n",
    "\n",
    "    # Create vector store index and query engine\n",
    "    query_engine = create_vector_store_index(nodes)\n",
    "\n",
    "    # Generate evaluation questions\n",
    "    df = generate_questions(nodes)\n",
    "\n",
    "    # Evaluate and aggregate results\n",
    "    eval_result = asyncio.run(evaluate_async(query_engine, df))\n",
    "    df_result = aggregate_results(df, eval_result)\n",
    "\n",
    "    # Print average scores\n",
    "    print_average_scores(df_result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (2) is close to chunk size (20). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|██████████| 6/6 [00:03<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctness scores: 4.5\n",
      "Faithfulness scores: 0.5\n",
      "Relevancy scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
